{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, Load and Transform Data\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Extract and prepare the Kaggle Stress Level Prediction dataset for analysis and modelling. Specifically:\n",
    "\n",
    "- Validate dataset structure.\n",
    "- Perform exploratory quality checks (missing values, duplicates, ranges).\n",
    "- Clean and standardise data.\n",
    "- Engineer features to improve interpretability and model performance.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- Raw Kaggle Stress Level Prediction Dataset `data/raw/stress_detection_data.csv` (originally from https://www.kaggle.com/datasets/shijo96john/stress-level-prediction)\n",
    "\n",
    "## What this notebook does\n",
    "\n",
    "1. Load and inspect the raw dataset.\n",
    "2. Conduct data quality checks (missing values, duplicates, range validation).\n",
    "3. Clean and standardise variables:\n",
    "   - Convert binary Yes/No features to integer-encoded (0/1).\n",
    "   - Parse `Bed_Time` and `Wake_Up_Time` strings into numeric format (minutes since midnight).\n",
    "   - Standardise categorical variables.\n",
    "4. Explore continuous feature distributions and detect skew/outliers.\n",
    "5. Perform feature engineering:\n",
    "   - Calculate sleep duration from bed/wake times (as validation).\n",
    "   - Create interaction features capturing potential stress risk patterns.\n",
    "   - Bin continuous variables into categories where useful.\n",
    "6. Export a cleaned, feature-enhanced dataset for use in modelling and EDA notebooks.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- Processed dataset to `data/processed/stress_data_processed.csv` for use in:\n",
    "  - `02_EDA.ipynb` (Exploratory Data Analysis Notebook)\n",
    "  - `03_Modelling.ipynb` (Modelling and Evaluation Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Change working directory\n",
    "\n",
    "I need to change the working directory from the current folder to its parent folder (required because the notebook is being run from inside the jupyter notebooks subfolder). In the code below, I change the working directory from its current folder to its parent folder.\n",
    "\n",
    "- I access the current directory with `os.getcwd()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory.\n",
    "\n",
    "- `os.path.dirname()` gets the parent directory\n",
    "- `os.chdir()` defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Data Load\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "This notebook works with the Stress Level Prediction dataset from Kaggle. The dataset contains self-reported lifestyle, sleep, and health data from individuals, with a target variable indicating their stress level.\n",
    "\n",
    "**Dataset Details:**\n",
    "- **Source:** Kaggle - \"Stress Level Prediction\" (shijo96john)\n",
    "- **Rows / Columns:** 773 rows x 22 columns\n",
    "- **Target:** `Stress_Detection` (Low / Medium / High)\n",
    "\n",
    "**Feature Categories:**\n",
    "- **Demographics:** Age, Gender, Marital_Status, Occupation\n",
    "- **Sleep:** Sleep_Duration, Sleep_Quality, Bed_Time, Wake_Up_Time\n",
    "- **Lifestyle:** Physical_Activity, Screen_Time, Caffeine_Intake, Alcohol_Intake, Smoking_Habit, Work_Hours, Travel_Time, Social_Interactions, Meditation_Practice, Exercise_Type\n",
    "- **Health Indicators:** Blood_Pressure, Cholesterol_Level, Blood_Sugar_Level\n",
    "\n",
    "### In this section:\n",
    "\n",
    "1. Load the raw dataset from `data/raw/stress_detection_data.csv`\n",
    "2. Conduct basic shape and structure checks\n",
    "3. Verify target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and the raw dataset\n",
    "\n",
    "# Core file handling libraries\n",
    "from pathlib import Path  # load the pathlib library\n",
    "\n",
    "# Data manipulation and analysis libraries\n",
    "import numpy as np  # load the numpy library\n",
    "import pandas as pd  # load the pandas library\n",
    "\n",
    "# Data visualisation libraries\n",
    "import matplotlib.pyplot as plt  # load the matplotlib library\n",
    "import seaborn as sns  # load the seaborn library\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)  # show all columns\n",
    "\n",
    "# Load the raw dataset\n",
    "df = pd.read_csv('data/raw/stress_detection_data.csv')  # load the dataset\n",
    "\n",
    "df.shape  # display the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()  # display the first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display target variable distribution\n",
    "stress_counts = df['Stress_Detection'].value_counts()  # count stress levels\n",
    "total_rows = df.shape[0]  # get total number of rows\n",
    "\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(\"-\" * 40)\n",
    "for level in stress_counts.index:\n",
    "    count = stress_counts[level]\n",
    "    pct = (count / total_rows) * 100\n",
    "    print(f\"{level}: {count} ({pct:.1f}%)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total: {total_rows} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Data Structure and Quality Check\n",
    "\n",
    "Basic data and structure information checks for the raw dataset:\n",
    "\n",
    "- Check dataframe shape\n",
    "- Check data types\n",
    "- Check for missing values\n",
    "- Check for duplicates\n",
    "\n",
    "**Expected Column Types:**\n",
    "\n",
    "**Numeric Continuous:**\n",
    "- Age, Sleep_Duration, Sleep_Quality, Physical_Activity, Screen_Time\n",
    "- Caffeine_Intake, Alcohol_Intake, Work_Hours, Travel_Time, Social_Interactions\n",
    "- Blood_Pressure, Cholesterol_Level, Blood_Sugar_Level\n",
    "\n",
    "**Categorical:**\n",
    "- Gender, Occupation, Marital_Status, Exercise_Type, Stress_Detection\n",
    "\n",
    "**Binary (Yes/No):**\n",
    "- Smoking_Habit, Meditation_Practice\n",
    "\n",
    "**Time Strings (to be parsed):**\n",
    "- Bed_Time, Wake_Up_Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic structure of the dataframe and quality check\n",
    "\n",
    "print(\"\\nColumn information:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check data types, non-null counts and missing values and create summary table\n",
    "summary = (\n",
    "    pd.DataFrame({\n",
    "        'Data Type': df.dtypes,  # data types\n",
    "        'Non-Null Count': df.notnull().sum(),  # non-null counts\n",
    "        'Missing Values': df.isnull().sum(),  # missing values\n",
    "        'Missing %': (df.isna().mean() * 100).round(3),  # percent missing\n",
    "        'Unique Values': df.nunique()  # unique values\n",
    "    })\n",
    "    .sort_values(by='Missing Values', ascending=False)  # sort by missing values\n",
    ")\n",
    "\n",
    "summary  # display the summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()  # count duplicate rows\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    print(f\"\\nDuplicate rows will be removed in the cleaning step.\")\n",
    "else:\n",
    "    print(\"No duplicate rows found - data is clean in this respect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Range Validation\n",
    "\n",
    "Before analysis, key continuous variables are checked below to ensure the values are sensible and within expected limits. This step helps confirm there are no invalid or extreme values (such as negative durations or unrealistic health indicators) that could distort analysis results.\n",
    "\n",
    "**Expected Ranges:**\n",
    "- Age: 18-100 (reasonable adult range)\n",
    "- Sleep_Duration: 0-24 hours\n",
    "- Sleep_Quality: 1-5 scale\n",
    "- Screen_Time: 0-24 hours\n",
    "- Work_Hours: 0-24 hours per day\n",
    "- Blood_Pressure: 80-200 (systolic)\n",
    "- Cholesterol_Level: 100-400\n",
    "- Blood_Sugar_Level: 50-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate ranges are sensible for continuous variables\n",
    "print(\"\\nValue Range Validation (Continuous Variables):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "cont_cols = [\n",
    "    \"Age\", \"Sleep_Duration\", \"Sleep_Quality\", \"Physical_Activity\",\n",
    "    \"Screen_Time\", \"Caffeine_Intake\", \"Alcohol_Intake\", \"Work_Hours\",\n",
    "    \"Travel_Time\", \"Social_Interactions\", \"Blood_Pressure\",\n",
    "    \"Cholesterol_Level\", \"Blood_Sugar_Level\"\n",
    "]  # list of continuous variable columns\n",
    "\n",
    "for col in cont_cols:\n",
    "    min_val = df[col].min()  # calculate minimum value\n",
    "    max_val = df[col].max()  # calculate maximum value\n",
    "    print(f\"{col:25s}: min={min_val:8.2f}, max={max_val:8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any negative values (should be none for most features)\n",
    "print(\"\\nNegative Value Counts (Continuous Variables):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "negative_counts = (df[cont_cols] < 0).sum()  # count negative values\n",
    "\n",
    "if negative_counts.sum() > 0:\n",
    "    print(negative_counts[negative_counts > 0])  # display counts of negative values\n",
    "else:\n",
    "    print(\"All continuous variables have valid non-negative values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Descriptive Statistics\n",
    "\n",
    "### Aim\n",
    "\n",
    "To get an initial sense of the dataset by reviewing averages, middle values, variation, and any obvious extremes before conducting more detailed plots.\n",
    "\n",
    "### Approach\n",
    "\n",
    "`df.describe()` was used to generate summary statistics for the numeric variables. Adding `include=\"all\"` includes non-numeric (categorical and object) columns in the summary, providing a more complete overview of the dataset.\n",
    "\n",
    "This summary displays key statistics such as the mean (average), median (middle point), standard deviation (spread of values), and percentiles (positions within the distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats for numeric variables\n",
    "df.describe()  # display summary statistics for numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats for categorical variables\n",
    "df.describe(include='object')  # display summary statistics for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Distribution and Outlier Checks\n",
    "\n",
    "### Aim\n",
    "\n",
    "To examine the continuous features to understand their distribution, scale, and the presence of extreme values. This will help decide whether transformations (such as log scaling) or outlier handling is needed later.\n",
    "\n",
    "### Approach\n",
    "\n",
    "- Plot histograms to view the shape and skew of each variable.\n",
    "- Use boxplots to spot potential outliers.\n",
    "- Apply the IQR (interquartile range) method to count possible outlier values numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for key continuous variables\n",
    "key_cont_cols = [\n",
    "    \"Sleep_Duration\", \"Sleep_Quality\", \"Screen_Time\",\n",
    "    \"Work_Hours\", \"Physical_Activity\", \"Caffeine_Intake\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(key_cont_cols):\n",
    "    axes[i].hist(df[col], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle(\"Distribution of Key Continuous Features\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for continuous variables to check for outliers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(key_cont_cols):\n",
    "    axes[i].boxplot(df[col].dropna())\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "plt.suptitle(\"Boxplots of Key Continuous Features\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR method to identify outliers\n",
    "print(\"\\nOutlier Detection using IQR Method:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for col in cont_cols:\n",
    "    Q1 = df[col].quantile(0.25)  # first quartile\n",
    "    Q3 = df[col].quantile(0.75)  # third quartile\n",
    "    IQR = Q3 - Q1  # interquartile range\n",
    "    lower_bound = Q1 - 1.5 * IQR  # lower bound for outliers\n",
    "    upper_bound = Q3 + 1.5 * IQR  # upper bound for outliers\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]  # identify outliers\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"{col:25s}: {len(outliers)} outliers (IQR method)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Categorical and Binary Feature Checks\n",
    "\n",
    "### Goal\n",
    "\n",
    "Confirm that categorical and binary variables are correctly formatted and assess their distributions. This ensures data integrity and highlights any imbalance.\n",
    "\n",
    "### Approach\n",
    "\n",
    "- Use `.value_counts()` to check the frequency of categories.\n",
    "- Create bar charts for visual confirmation.\n",
    "- Convert Yes/No binary variables to integer (0/1) for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical feature checks\n",
    "cat_cols = [\"Gender\", \"Marital_Status\", \"Exercise_Type\", \"Occupation\"]\n",
    "\n",
    "print(\"Categorical Feature Value Counts:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary feature checks (Yes/No)\n",
    "binary_cols = [\"Smoking_Habit\", \"Meditation_Practice\"]\n",
    "\n",
    "print(\"\\nBinary Feature Value Counts (Yes/No):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in binary_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plots for categorical variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    value_counts = df[col].value_counts()\n",
    "    axes[i].bar(value_counts.index, value_counts.values, edgecolor='black')\n",
    "    axes[i].set_title(f'{col} Distribution')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time string checks (Bed_Time and Wake_Up_Time)\n",
    "print(\"\\nTime Variable Samples:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nBed_Time unique values (first 10):\")\n",
    "print(df['Bed_Time'].unique()[:10])\n",
    "print(\"\\nWake_Up_Time unique values (first 10):\")\n",
    "print(df['Wake_Up_Time'].unique()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Correlation Analysis\n",
    "\n",
    "Heatmap to explore relationships between numeric variables and see how strongly they correlate with each other. This helps identify redundancy between features and provides an early indication of predictive signal.\n",
    "\n",
    "**Note:** Correlation with the target variable (`Stress_Detection`) will be explored after encoding it numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap for numeric features\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr = df[numeric_cols].corr()  # calculate the correlation matrix\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True)\n",
    "plt.title(\"Correlation Heatmap - Numeric Features\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Data Cleaning and Feature Engineering\n",
    "\n",
    "### Goal\n",
    "\n",
    "Clean the dataset and create new variables/transformations to capture stress patterns that are not easily visible in the raw features. This step improves model performance and interpretability.\n",
    "\n",
    "### Approach\n",
    "\n",
    "1. **Convert Yes/No binary features** to integer (0/1) encoding.\n",
    "2. **Parse Bed_Time and Wake_Up_Time** strings into numeric format (minutes since midnight).\n",
    "3. **Encode target variable** (`Stress_Detection`) ordinally (Low=0, Medium=1, High=2).\n",
    "4. **Create interaction features** to capture combined effects.\n",
    "5. **Bin continuous variables** into categories where useful for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# 1. Convert Yes/No binary features to integer (0/1)\n",
    "binary_yes_no_cols = [\"Smoking_Habit\", \"Meditation_Practice\"]\n",
    "\n",
    "for col in binary_yes_no_cols:\n",
    "    df_processed[col] = df_processed[col].map({'Yes': 1, 'No': 0}).astype(int)\n",
    "\n",
    "print(\"Binary features converted to 0/1:\")\n",
    "print(df_processed[binary_yes_no_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Parse Bed_Time and Wake_Up_Time to minutes since midnight\n",
    "\n",
    "def time_to_minutes(time_str):\n",
    "    \"\"\"\n",
    "    Convert time string (e.g., '10:00 PM', '7:00 AM') to minutes since midnight.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time_str = time_str.strip()\n",
    "        # Parse the time using pandas\n",
    "        time_obj = pd.to_datetime(time_str, format='%I:%M %p')\n",
    "        return time_obj.hour * 60 + time_obj.minute\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the conversion\n",
    "df_processed['Bed_Time_Minutes'] = df_processed['Bed_Time'].apply(time_to_minutes)\n",
    "df_processed['Wake_Up_Time_Minutes'] = df_processed['Wake_Up_Time'].apply(time_to_minutes)\n",
    "\n",
    "print(\"\\nTime conversion results (first 5 rows):\")\n",
    "print(df_processed[['Bed_Time', 'Bed_Time_Minutes', 'Wake_Up_Time', 'Wake_Up_Time_Minutes']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify time conversion ranges\n",
    "print(\"\\nBed_Time_Minutes range:\")\n",
    "print(f\"Min: {df_processed['Bed_Time_Minutes'].min()}, Max: {df_processed['Bed_Time_Minutes'].max()}\")\n",
    "print(\"\\nWake_Up_Time_Minutes range:\")\n",
    "print(f\"Min: {df_processed['Wake_Up_Time_Minutes'].min()}, Max: {df_processed['Wake_Up_Time_Minutes'].max()}\")\n",
    "\n",
    "# Check for any NaN values introduced\n",
    "print(f\"\\nNaN values in Bed_Time_Minutes: {df_processed['Bed_Time_Minutes'].isna().sum()}\")\n",
    "print(f\"NaN values in Wake_Up_Time_Minutes: {df_processed['Wake_Up_Time_Minutes'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Encode target variable ordinally (Low=0, Medium=1, High=2)\n",
    "stress_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "df_processed['Stress_Level_Encoded'] = df_processed['Stress_Detection'].map(stress_mapping)\n",
    "\n",
    "print(\"\\nTarget variable encoding:\")\n",
    "print(df_processed[['Stress_Detection', 'Stress_Level_Encoded']].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create interaction features\n",
    "\n",
    "# Sleep efficiency proxy: Sleep Quality relative to Sleep Duration\n",
    "df_processed['Sleep_Efficiency'] = df_processed['Sleep_Quality'] / df_processed['Sleep_Duration']\n",
    "\n",
    "# Screen time to physical activity ratio\n",
    "df_processed['Screen_Activity_Ratio'] = df_processed['Screen_Time'] / (df_processed['Physical_Activity'] + 0.1)  # add small value to avoid division by zero\n",
    "\n",
    "# Work-Life balance indicator: Work_Hours + Travel_Time\n",
    "df_processed['Work_Travel_Total'] = df_processed['Work_Hours'] + df_processed['Travel_Time']\n",
    "\n",
    "# High screen time flag (above median)\n",
    "df_processed['High_Screen_Time'] = (df_processed['Screen_Time'] > df_processed['Screen_Time'].median()).astype(int)\n",
    "\n",
    "# Low sleep flag (below 6 hours)\n",
    "df_processed['Low_Sleep'] = (df_processed['Sleep_Duration'] < 6).astype(int)\n",
    "\n",
    "print(\"\\nInteraction features created:\")\n",
    "print(df_processed[['Sleep_Efficiency', 'Screen_Activity_Ratio', 'Work_Travel_Total', 'High_Screen_Time', 'Low_Sleep']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Bin continuous variables into categories\n",
    "\n",
    "# Sleep Duration bins\n",
    "df_processed['Sleep_Duration_Bin'] = pd.cut(\n",
    "    df_processed['Sleep_Duration'],\n",
    "    bins=[0, 5, 6, 7, 8, 24],\n",
    "    labels=['Very Low (<5h)', 'Low (5-6h)', 'Normal (6-7h)', 'Good (7-8h)', 'High (>8h)']\n",
    ")\n",
    "\n",
    "# Work Hours bins\n",
    "df_processed['Work_Hours_Bin'] = pd.cut(\n",
    "    df_processed['Work_Hours'],\n",
    "    bins=[0, 6, 8, 10, 24],\n",
    "    labels=['Low (<6h)', 'Normal (6-8h)', 'High (8-10h)', 'Very High (>10h)']\n",
    ")\n",
    "\n",
    "# Screen Time bins\n",
    "df_processed['Screen_Time_Bin'] = pd.cut(\n",
    "    df_processed['Screen_Time'],\n",
    "    bins=[0, 2, 4, 6, 24],\n",
    "    labels=['Low (<2h)', 'Moderate (2-4h)', 'High (4-6h)', 'Very High (>6h)']\n",
    ")\n",
    "\n",
    "print(\"\\nBinned variable distributions:\")\n",
    "print(\"\\nSleep Duration Bins:\")\n",
    "print(df_processed['Sleep_Duration_Bin'].value_counts())\n",
    "print(\"\\nWork Hours Bins:\")\n",
    "print(df_processed['Work_Hours_Bin'].value_counts())\n",
    "print(\"\\nScreen Time Bins:\")\n",
    "print(df_processed['Screen_Time_Bin'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all engineered features were created correctly\n",
    "\n",
    "print(\"Full feature set verification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# List ALL engineered features\n",
    "engineered_features = [\n",
    "    \"Bed_Time_Minutes\",\n",
    "    \"Wake_Up_Time_Minutes\",\n",
    "    \"Stress_Level_Encoded\",\n",
    "    \"Sleep_Efficiency\",\n",
    "    \"Screen_Activity_Ratio\",\n",
    "    \"Work_Travel_Total\",\n",
    "    \"High_Screen_Time\",\n",
    "    \"Low_Sleep\",\n",
    "    \"Sleep_Duration_Bin\",\n",
    "    \"Work_Hours_Bin\",\n",
    "    \"Screen_Time_Bin\"\n",
    "]\n",
    "\n",
    "# 1. Check if all features exist\n",
    "print(\"\\nFeature existence check:\")\n",
    "missing_features = [f for f in engineered_features if f not in df_processed.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"Missing features: {missing_features}\")\n",
    "else:\n",
    "    print(f\"All {len(engineered_features)} engineered features created successfully\")\n",
    "\n",
    "# 2. Feature count summary\n",
    "print(f\"\\nFeature count summary:\")\n",
    "original_features = 22\n",
    "new_features = len(engineered_features)\n",
    "total_features = df_processed.shape[1]\n",
    "\n",
    "print(f\"Original features: {original_features}\")\n",
    "print(f\"Engineered features: {new_features}\")\n",
    "print(f\"Total features in dataset: {total_features}\")\n",
    "\n",
    "# 3. Check for NaN values in engineered features\n",
    "print(f\"\\nNaN check in engineered features:\")\n",
    "nan_counts = df_processed[engineered_features].isna().sum()\n",
    "\n",
    "if nan_counts.sum() > 0:\n",
    "    print(f\"NaN values found:\")\n",
    "    print(nan_counts[nan_counts > 0])\n",
    "else:\n",
    "    print(f\"No NaN values in engineered features\")\n",
    "\n",
    "# 4. Data type verification\n",
    "print(f\"\\nDatatype verification for engineered features:\")\n",
    "for feat in engineered_features:\n",
    "    dtype = df_processed[feat].dtype\n",
    "    print(f\"{feat:30s}: {dtype}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Feature engineering verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Save Processed Dataset\n",
    "\n",
    "Save the cleaned and feature-engineered dataset so it can be reused in modelling and visualisation notebooks without re-running the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed folder if it does not exist\n",
    "processed_path = Path(\"data/processed\")  # define the path for the processed data folder\n",
    "processed_path.mkdir(parents=True, exist_ok=True)  # create the folder if it does not exist\n",
    "\n",
    "# Define output file path\n",
    "output_file = processed_path / \"stress_data_processed.csv\"  # define the output file path\n",
    "\n",
    "# Save processed file to CSV\n",
    "df_processed.to_csv(output_file, index=False)  # save the dataframe to a CSV file without the index\n",
    "\n",
    "print(f\"Processed file saved to {output_file}\")\n",
    "print(f\"Final Shape: {df_processed.shape}\")\n",
    "\n",
    "# Verify file exists and print confirmation\n",
    "if output_file.exists():\n",
    "    print(f\"File exists: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final column list for reference\n",
    "print(\"\\nFinal Dataset Columns:\")\n",
    "print(\"=\" * 70)\n",
    "for i, col in enumerate(df_processed.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Summary and Next Steps\n",
    "\n",
    "### Summary of work completed in this notebook:\n",
    "\n",
    "1. **Loaded** the raw Stress Level Prediction dataset from `data/raw/stress_detection_data.csv` and confirmed structure (773 rows x 22 columns).\n",
    "\n",
    "2. **Verified data quality:**\n",
    "   - No missing values detected\n",
    "   - No duplicate rows found\n",
    "   - All continuous variables have sensible ranges\n",
    "\n",
    "3. **Confirmed target variable distribution:**\n",
    "   - Medium: 310 (40.1%)\n",
    "   - High: 301 (38.9%)\n",
    "   - Low: 162 (21.0%)\n",
    "\n",
    "4. **Explored distributions:**\n",
    "   - Examined continuous variable distributions via histograms and boxplots\n",
    "   - Identified potential outliers using IQR method\n",
    "   - Checked categorical and binary feature distributions\n",
    "\n",
    "5. **Cleaned and transformed data:**\n",
    "   - Converted Yes/No binary features to integer (0/1)\n",
    "   - Parsed Bed_Time and Wake_Up_Time strings to minutes since midnight\n",
    "   - Encoded target variable ordinally (Low=0, Medium=1, High=2)\n",
    "\n",
    "6. **Engineered new features:**\n",
    "   - Sleep_Efficiency (Sleep_Quality / Sleep_Duration)\n",
    "   - Screen_Activity_Ratio (Screen_Time / Physical_Activity)\n",
    "   - Work_Travel_Total (Work_Hours + Travel_Time)\n",
    "   - High_Screen_Time flag (above median)\n",
    "   - Low_Sleep flag (below 6 hours)\n",
    "   - Binned variables: Sleep_Duration_Bin, Work_Hours_Bin, Screen_Time_Bin\n",
    "\n",
    "7. **Exported** the processed dataset (773 rows) to `data/processed/stress_data_processed.csv`.\n",
    "\n",
    "### Insights from ETL & Feature Engineering:\n",
    "\n",
    "- The target variable is fairly balanced across the three stress levels, with a slight skew towards Medium and High stress.\n",
    "- Sleep duration and quality show variation that may be associated with stress levels.\n",
    "- Screen time and work hours show reasonable distributions with some high values.\n",
    "- Binary features (Smoking_Habit, Meditation_Practice) are now properly encoded for modelling.\n",
    "- Time features (Bed_Time, Wake_Up_Time) are now in numeric format for analysis.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Exploratory Data Analysis (02_EDA.ipynb):**\n",
    "   - Conduct hypothesis tests (H1-H8) as outlined in the README\n",
    "   - Visualise relationships between features and stress levels\n",
    "   - Calculate statistical significance and effect sizes\n",
    "\n",
    "2. **Modelling (03_Modelling.ipynb):**\n",
    "   - Train baseline models (Logistic Regression, Decision Tree)\n",
    "   - Evaluate using Accuracy, Macro F1, confusion matrix\n",
    "   - Feature importance analysis\n",
    "   - Model comparison and selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
